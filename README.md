# ğŸ” Explainable BERT (Under Development)

âš ï¸ **NOTE:** This is a custom research-driven project aiming to enhance explainability in Transformer-based architectures. A minor but **novel architectural change** has been made to standard BERT in order to **highlight the tokens most responsible for a given prediction** (e.g., in sentiment classification).

ğŸ’¡ The **core modification** will be detailed after full development is complete and the model is evaluated extensively. A **prototype implementation** has already shown **promising results** â€” improving interpretability while maintaining or even increasing accuracy over the baseline.

ğŸ› ï¸ This repo is currently in active development. Stay tuned!

---

## ğŸš€ Goals

- Build a lightweight BERT model with built-in interpretability
- Highlight token-level contributions during inference
- Compare against standard multi-head attention strategies
- Open source the implementation after stability & documentation


